# SISVAN data

```{r}
#| label: setup
#| include: false

source(here::here("R/quarto-setup.R"))
```

```{r}
#| echo: false
#| output: asis

rutils:::quarto_status(
  type = "drafting",
  of_what = "of this report"
  )
```

::: {.callout-note}
Use the [`targets`](https://docs.ropensci.org/targets/) package while processing the data.
:::

::: {.callout-note}
See chapter 22 from the book _R for Data Science_ by Hadley Wickham and Garrett Grolemund. <https://r4ds.hadley.nz/arrow>.
:::

::: {.callout-warning}
Code evalution was disabled for the code chunks below.
:::

## Dowloading and unzipping the data

::: {.callout-note}
See:

* <https://opendatasus.saude.gov.br/dataset/sisvan-estado-nutricional>
* <https://apidadosabertos.saude.gov.br/v1/#/SISVAN/get_sisvan_estado_nutricional>
:::

```{r}
#| eval: false
#| code-fold: false

# library(arrow, quietly = TRUE)
# library(curl, quietly = TRUE)
# library(dplyr, quietly = TRUE)
# library(utils, quietly = TRUE)
```

```{r}
#| eval: false
#| code-fold: false

dir_path <- here::here(".data", "sisvan", "estado-nutricional", "2023", "zip")

if (!dir.exists(dir_path)) {
  dir.create(dir_path, recursive = TRUE, showWarnings = FALSE)
}

url <- paste0(
  "https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/",
  "SISVAN/estado_nutricional/sisvan_estado_nutricional_2023.zip"
)

dest_file <- here::here(dir_path, "sisvan_estado-nutricional_2023.zip")
curl::curl_download(url = url, destfile = dest_file)

dir_path <- dir_path |> stringr::str_replace("zip$", "csv")
if (!dir.exists(dir_path)) dir.create(dir_path, showWarnings = FALSE)

file <- utils::unzip(zipfile = dest_file, exdir = dir_path)
```

## Creating a schema for the data

```{r}
#| eval: false
#| code-fold: false

schema <- arrow::schema(
  CO_ACOMPANHAMENTO = arrow::string(),
  CO_PESSOA_SISVAN = arrow::string(),
  ST_PARTICIPA_ANDI = arrow::string(),
  CO_MUNICIPIO_IBGE = arrow::string(), # arrow::int64()
  SG_UF = arrow::string(),
  NO_MUNICIPIO = arrow::string(),
  CO_CNES = arrow::string(), # arrow::int64()
  NU_IDADE_ANO = arrow::int64(),
  NU_FASE_VIDA = arrow::float(),
  DS_FASE_VIDA = arrow::string(),
  SG_SEXO = arrow::string(),
  CO_RACA_COR = arrow::int64(),
  DS_RACA_COR = arrow::string(),
  CO_POVO_COMUNIDADE = arrow::int64(),
  DS_POVO_COMUNIDADE = arrow::binary(),
  CO_ESCOLARIDADE = arrow::int64(),
  DS_ESCOLARIDADE = arrow::binary(),
  DT_ACOMPANHAMENTO = arrow::string(),
  NU_COMPETENCIA = arrow::int64(),
  NU_PESO = arrow::string(),
  NU_ALTURA = arrow::string(),
  DS_IMC = arrow::string(),
  DS_IMC_PRE_GESTACIONAL = arrow::float(),
  "PESO X IDADE" = arrow::string(),
  "PESO X ALTURA" = arrow::string(),
  "CRI. ALTURA X IDADE" = arrow::string(),
  "CRI. IMC X IDADE" = arrow::string(),
  "ADO. ALTURA X IDADE" = arrow::string(),
  "ADO. IMC X IDADE" = arrow::string(),
  CO_ESTADO_NUTRI_ADULTO = arrow::binary(),
  CO_ESTADO_NUTRI_IDOSO = arrow::binary(),
  CO_ESTADO_NUTRI_IMC_SEMGEST = arrow::binary(),
  CO_SISTEMA_ORIGEM_ACOMP = arrow::int64(),
  SISTEMA_ORIGEM_ACOMP = arrow::string()
)

# arrow::schema(arrow_data)
```

## Creating an `arrow` dataset based on the CSV file

```{r}
#| eval: false
#| code-fold: false

arrow_data <-
  file |>
  arrow::open_delim_dataset(
    schema = NULL,
    partitioning = arrow::hive_partition(),
    hive_style = NA,
    unify_schemas = NULL,
    factory_options = list(),
    delim = ";",
    quote = "\"",
    escape_double = TRUE,
    escape_backslash = FALSE,
    col_names = TRUE,
    col_types = schema,
    na = c("", "NA"),
    skip_empty_rows = TRUE,
    skip = 0L,
    convert_options = NULL,
    read_options = NULL,
    timestamp_parsers = NULL,
    quoted_na = TRUE,
    parse_options = NULL
  )
```

## Transforming the data to parquet

```{r}
#| eval: false
#| code-fold: false

dir_path <- dir_path |> stringr::str_replace("csv$", "parquet")
if (!dir.exists(dir_path)) dir.create(dir_path, showWarnings = FALSE)

# Adjust the parameters below according to your machine's capacity.
arrow_data |>
  dplyr::group_by(SG_UF) |>
  arrow::write_dataset(
    path = dir_path,
    format = "parquet",
    max_partitions = 2000,
    max_open_files = 100,
    max_rows_per_file = 100000
  )

dplyr::tibble(
  files = list.files(dir_path, recursive = TRUE),
  size_MB = file.size(file.path(dir_path, files)) / 1024^2
)
```

## Creating an `arrow` dataset based on the parquet files

```{r}
#| eval: false
#| code-fold: false

dir_path <- here::here(".data/sisvan_estado-nutricional-2023/parquet")
if (!dir.exists(dir_path)) dir.create(dir_path, showWarnings = FALSE)

arrow_data |>
  dplyr::group_by(SG_UF) |>
  arrow::write_dataset(
    path = dir_path,
    format = "parquet",
    max_partitions = 2000,
    max_open_files = 100,
    max_rows_per_file = 100000
  )

dplyr::tibble(
  files = list.files(dir_path, recursive = TRUE),
  size_MB = file.size(file.path(dir_path, files)) / 1024^2
)
```

```{r}
#| eval: false
#| code-fold: false

arrow_data <- arrow::open_dataset(dir_path)
```

## Example of data processing with an `arrow` dataset

```{r}
#| eval: false
#| code-fold: false

# library(geobr, quietly = TRUE)
# library(sidrar, quietly = TRUE)
# library(rutils, quietly = TRUE)
# library(viridis, quietly = TRUE)

source(here::here("R", "utils.R"))
```

```{r}
#| eval: false
#| code-fold: false

data <-
  arrow_data |>
  # dplyr::filter(SG_UF == "AC") |>
  dplyr::select(SG_UF, `PESO X IDADE`, `PESO X ALTURA`) |>
  dplyr::group_by(SG_UF, `PESO X IDADE`) |>
  dplyr::summarise(n = dplyr::n()) |>
  dplyr::collect()

data |> dplyr::glimpse()
```

```{r}
#| eval: false
#| code-fold: false

data <-
  data %>%
  rutils:::change_name(tidy_names(names(.), tag_duplicates = TRUE)) |>
  dplyr::filter(peso_x_idade == "Muito baixo peso para a idade") |>
  dplyr::rename(state = sg_uf)
```

::: {.callout-note}
See:

* <https://apisidra.ibge.gov.br/>
* <https://sidra.ibge.gov.br/tabela/7358>
:::

```{r}
#| eval: false
#| code-fold: false

sidra_data <-
  sidrar::get_sidra(
    api = "/t/7358/n1/all/n3/all/v/all/p/all/c2/6794/c287/100362/c1933/49039"
  )

sidra_data <-
  sidra_data %>%
  rutils:::change_name(tidy_names(names(.), tag_duplicates = FALSE)) |>
  dplyr::as_tibble() |>
  dplyr::rename(
    state = brasil_e_unidade_da_federacao,
    population = valor,
    year = ano_dup_2,
  ) |>
  dplyr::filter(nivel_territorial == "Unidade da Federação") |>
  dplyr::select(state, population, year) |>
  dplyr::mutate(state = rename_br_state_to_uf(state))
```

```{r}
#| eval: false
#| code-fold: false

data <-
  data |>
  dplyr::left_join(sidra_data, by = "state") |>
  dplyr::mutate(
    n_rel = n / population,
    n_per = n_rel * 100
  ) |>
  dplyr::arrange(dplyr::desc(n_per))
```

```{r}
#| eval: false
#| code-fold: false

brazil_uf_map <-
  geobr::read_state(year = 2020, showProgress = FALSE) |>
  rutils:::shush() |>
  dplyr::rename(state = abbrev_state) |>
  dplyr::select(state, geom)

data <-
  data |>
  dplyr::left_join(brazil_uf_map, by = "state")
```

```{r}
#| eval: false
#| code-fold: false

data |>
  dplyr::mutate(n = n_per) |>
  dplyr::select(state, n, geom) |>
  ggplot2::ggplot() +
  ggplot2::geom_sf(ggplot2::aes(geometry = geom, fill = n)) +
  ggplot2::labs(
    title = "SISVAN 2023",
    subtitle = paste0(
      "Porcentagem de crianças com 'muito baixo peso para a idade' ",
      "em relação à\npopulação total da UF. "
    ),
    x = "Longitude",
    y = "Latitude",
    fill = "%"
  ) +
  viridis::scale_fill_viridis(option = "inferno")
```

## Getting samples of the data with `readr`

```{r}
#| eval: false
#| code-fold: false

# library(readr, quietly = TRUE)

source(here::here("R", "read_demas_sisvan.R"))
```

```{r}
#| eval: false
#| code-fold: false

data <-
  file |>
  read_demas_sisvan(
    skip = 0,
    n_max = 1000,
    lazy = FALSE,
    show_progress = TRUE
  )

data |> dplyr::glimpse()
```

```{r}
#| eval: false
#| code-fold: false

data <-
  file |>
  read_and_filter(
    fns = read_demas_sisvan,
    filter_statement = "SG_UF == 'SP'",
    batch_n = 10000,
    batch_max = 50,
    has_header = TRUE
  )

data |> dplyr::glimpse()
```
